# Module 4 Assignment 2 Random Forests

## Heinrich Wyschka


```{r, include = FALSE}
library(tidyverse)
library(tidymodels)
library(caret)
library(gridExtra)
library(ranger)
library(randomForest)
library(vip)
library(VIM)
library(mice)
library(skimr)
```

```{r}
drug = read_csv("drug_data-1.csv")
```

Naming the columns:
```{r}

names(drug) = c("ID", "Age", "Gender", "Education", "Country", "Ethnicity",
"Nscore", "Escore", "Oscore", "Ascore", "Cscore", "Impulsive",
"SS", "Alcohol", "Amphet", "Amyl", "Benzos", "Caff", "Cannabis",
"Choc", "Coke", "Crack", "Ecstasy", "Heroin", "Ketamine", "Legalh",
"LSD", "Meth", "Mushrooms", "Nicotine", "Semer", "VSA")
#str(drug)
```

change values to Nos and Yess
```{r}
drug[drug == "CL0"] = "No"
drug[drug == "CL1"] = "No"
drug[drug == "CL2"] = "Yes"
drug[drug == "CL3"] = "Yes"
drug[drug == "CL4"] = "Yes"
drug[drug == "CL5"] = "Yes"
drug[drug == "CL6"] = "Yes"
```

```{r}
drug_clean = drug %>% mutate_at(vars(Age:Ethnicity), funs(as_factor)) %>%
                      mutate(Age = factor(Age, labels = c("18_24", "25_34", "35_44","45_54", "55_64","65_"))) %>%
mutate(Gender = factor(Gender, labels = c("Male", "Female"))) %>%
mutate(Education = factor(Education, labels =
c("Under16", "At16", "At17", "At18", "SomeCollege",
"ProfessionalCert", "Bachelors", "Masters", "Doctorate"))) %>%
mutate(Country = factor(Country,
labels = c("USA", "NewZealand", "Other", "Australia",
"Ireland","Canada","UK"))) %>%
mutate(Ethnicity = factor(Ethnicity,
labels = c("Black", "Asian", "White", "White/Black", "Other",
"White/Asian", "Black/Asian"))) %>%
mutate_at(vars(Alcohol:VSA), funs(as_factor)) %>%
select(-ID)
#str(drug_clean)
```


```{r}
drug_clean = drug_clean %>% select(!(Alcohol:Mushrooms)) %>% select(!(Semer:VSA))
names(drug_clean)
```


### Task 1

The accompanying website (http://archive.ics.uci.edu/ml/datasets/Drug+consumption+%28quantified%29) says that there are no missing values, but let's check:

```{r}
summary(drug_clean)
skim(drug_clean)
```

No, 0 missing pieces of data. Good.


### Task 2


```{r}
set.seed(1234)
drug_split = initial_split(drug_clean, prop = 0.70, strata = Nicotine)
train = training(drug_split)
test = testing(drug_split)
```


### Task 3

```{r}
p1 = ggplot(train, aes(x = Age, fill = Nicotine)) + geom_bar(position = "fill")
p2 = ggplot(train, aes(x = Gender, fill = Nicotine)) + geom_bar(position = "fill")
p3 = ggplot(train, aes(x = Education, fill = Nicotine)) + geom_bar(position = "fill")
grid.arrange(p1,p2,p3)
```

Nicotine and...
- Age: It looks like the older you get the less likely you comsume Nicotine.
- Gender: Females are less likely to use Nicotine
- Education (left school at...): If you left school under age 16 there is very likely you used nicotine, at 16 less likely which increases at age 17 and 18, then having some college education is about the same as leaving school at 18. From there on the likelihood decreases for you to use nicotine. Having a doctorate is the least likely to consume nicotine. 


```{r}
p4 = ggplot(train, aes(x = Country, fill = Nicotine)) + geom_bar(position = "fill")
p5 = ggplot(train, aes(x = Ethnicity, fill = Nicotine)) + geom_bar(position = "fill")
grid.arrange(p4,p5)
```

- county: in order from most likely to use nicotine to least likely: Ireland, USA/other, New Zealand, Canada, UK.
- ethnicity: in order from most likely to consume nicotine to least likely: White/Black and White/Asian are the lowest, then Others, Black/Asian, White and finally seemingly the same: Black and Asian.

```{r}
p7 = ggplot(train, aes(x = Nicotine, y = Escore)) + geom_boxplot()
p6 = ggplot(train, aes(x = Nicotine, y = Nscore)) + geom_boxplot()
p8 = ggplot(train, aes(x = Nicotine, y = Oscore)) + geom_boxplot()
grid.arrange(p6,p7,p8, ncol = 2)
```

- Nscore and Oscore look very similar. The Nscore and Oscore's medians are higher when consuming nicotine. The Escore is nearly identical.   


```{r}
p9 = ggplot(train, aes(x = Nicotine, y = Ascore)) + geom_boxplot()
p10 = ggplot(train, aes(x = Nicotine, y = Cscore)) + geom_boxplot()
p11 = ggplot(train, aes(x = Nicotine, y = Impulsive)) + geom_boxplot()
p12 = ggplot(train, aes(x = Nicotine, y = SS)) + geom_boxplot()
grid.arrange(p9,p10,p11,p12, ncol = 2)
```

- Ascores and Cscores' medians are the opposite. Here "No" is higher than "Yes". 
- Impulsive: The more impulsive you are the more likely is that you consume nicotine.
- SS: It is clear that this measure favors nicotine consumption.


### Task 4

Create a random forest model on the training set to predict Nicotine using all of the variables in the
dataset. You 5-fold, k-fold cross-validation (random number seed of 123 for the folds). Allow R to select mtry
values between 2 and 8 and min_n values between 5 and 20. Use 10 levels in your “grid_regular” function.
Set a random number seed of 123 for the tune_grid function. Use 100 trees.
Visualize the relationships between parameters and performance metrics.


```{r}
set.seed(123)
rf_folds = vfold_cv(train, v = 5)
```


```{r}
drug_recipe = recipe(Nicotine ~., train) %>%
  step_dummy(all_nominal(), -all_outcomes())

rf_model = rand_forest(mtry = tune(), min_n = tune(), trees = 100) %>% #add tuning of mtry and min_n parameters
  #setting trees to 100 here should also speed things up a bit, but more trees might be better
  set_engine("ranger", importance = "permutation") %>% #added importance metric
  set_mode("classification")

drug_wflow = 
  workflow() %>% 
  add_model(rf_model) %>% 
  add_recipe(drug_recipe)

rf_grid = grid_regular(
  mtry(range = c(2, 8)), #these values determined through significant trial and error
  min_n(range = c(5, 20)), #these values determined through significant trial and error
  levels = 10
)

set.seed(123)
rf_res_tuned = tune_grid(
  drug_wflow,
  resamples = rf_folds,
  grid = rf_grid #use the tuning grid
)
```



```{r}
rf_res_tuned %>%
  collect_metrics() %>%
  filter(.metric == "accuracy") %>%
  select(mean, min_n, mtry) %>%
  pivot_longer(min_n:mtry,
    values_to = "value",
    names_to = "parameter"
  ) %>%
  ggplot(aes(value, mean, color = parameter)) +
  geom_point(show.legend = FALSE) +
  facet_wrap(~parameter, scales = "free_x") +
  labs(x = NULL, y = "Accuracy")
```
An alternate view of the parameters  
```{r}
rf_res_tuned %>%
  collect_metrics() %>%
  filter(.metric == "accuracy") %>%
  mutate(min_n = factor(min_n)) %>%
  ggplot(aes(mtry, mean, color = min_n)) +
  geom_line(alpha = 0.5, size = 1.5) +
  geom_point() +
  labs(y = "Accuracy")
```


### Task 5

Use the best mtry and min_n values from Task 4 to finalize the workflow and fit the model to
training set. Examine variable importance. What variables are most important in this model? (Hint: Refer
back to the dataset’s webpage if you need clarification as to meaning of any variables).

```{r}
best_rf = select_best(rf_res_tuned, "accuracy")

final_rf = finalize_workflow(
  drug_wflow,
  best_rf
)

final_rf
```
```{r}
#fit the finalized workflow to our training data
final_rf_fit = fit(final_rf, train)
```


Check out variable importance
```{r}
final_rf_fit %>% pull_workflow_fit() %>% vip(geom = "point")
```

"SS" stands for "Sensation Seeking", which is the most important for the model I created BY FAR >0.015. The next for are between 0.004 and 0.008 in this order: UK, ppenness to experience, age 45-54 and conscientiousness.


### Task 6

Performance: Training set
```{r}
trainpredrf = predict(final_rf_fit, train)
confusionMatrix(trainpredrf$.pred_class, train$Nicotine, 
                positive = "Yes")
```

Prediction: testing set
```{r}
testpredrf = predict(final_rf_fit, test)
confusionMatrix(testpredrf$.pred_class, test$Nicotine, 
                positive = "Yes")
```

The training performance has a n accuracy of 84.32%. As expected the testing set is lower with an accuracy of 71.15%; this is over 13% different. I am not too happy with these numbers; I would rather see a difference below 10%.

### Task 7

Since we did this data on nicotine (smoking cigarettes or vapeing) we could categorize individuals, that might get evaluated for substance abuse, into possible smoker and non-smokers. This could affect what treatment plan they will be prescribed, how much resources should be allocated or what housing to send this individual to. I would not use this model that provided me with only about 71% accuracy on the testing data (which is supposed to be "real world"). Yes, I would be concerned using this model. This is medical data which is closest connected to life and death questions; we are not trying to identify shoppers for the newest iPhone and which color they will choose.     